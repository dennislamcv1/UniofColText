{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcU5XMxHW35O"
   },
   "source": [
    "# MSDS Network Analysis, Lab 2: Build a Semantic Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9OaEUqP43Bz"
   },
   "source": [
    "## ‚ö°Ô∏è Make a Copy\n",
    "\n",
    "Save a copy of this notebook in your Google Drive before continuing. Be sure to edit your own copy, not the original notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRg_nVYGXIVh"
   },
   "source": [
    "## üìì About this lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0FWKf20XNAT"
   },
   "source": [
    "In this lab, you will build a semantic network of Tweets. That is, a graph of Tweets related by natural language features of the Tweet texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gNOVxdOLfr1"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEr-dIfv90fP"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import re\n",
    "import itertools\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qLFOpRZZjuh"
   },
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1oAfXlkZeOX"
   },
   "source": [
    "Be sure you still have the brand Tweets file on your Google Drive from the previous Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdigBPlKTDeS"
   },
   "outputs": [],
   "source": [
    "DATA_FILE = \"nikelululemonadidas_tweets.jsonl.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLsas-r3p6ws"
   },
   "source": [
    "## Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjOJUUbdp9EJ"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnNJ_pDUXCEq"
   },
   "source": [
    "### About the NLTK and NLP in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MwcyFx7w9VW"
   },
   "source": [
    "The [NLTK (Natural Language Toolkit)](https://www.nltk.org/) is an old standby for natural language processing (NLP) in the Python world.\n",
    "\n",
    "There are a good number of NLP-related Python packages, but many of them are in fact built on the NLTK, so it is worth getting some foundational exposure to that package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plWCMGaqx15Q"
   },
   "source": [
    "If you want to explore NLP with Python in more depth, popular libraries include [spaCy](https://spacy.io/) and [TextBlob](https://textblob.readthedocs.io/en/dev/) both of which are (like the NLTK) broadly scoped generalist NLP libraries.\n",
    "\n",
    "There are also a good number of specialized libraries for a number of tasks, including keyword extraction, fuzzy string matching, natural language data handling, the list goes on. If you have a task at hand, it's worth doing a quick search to see if the problem has already been solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVBYutrh0jJd"
   },
   "source": [
    "### NLTK downloads\n",
    "\n",
    "A lot of NLTK tools fall into the category of corpus linguistics, and the algorithms for these tools often require large amounts of backing data that can unnecessarily bloat the package size if that tool is not being used.\n",
    "\n",
    "To help manage bloat, the NLTK distributes its supplemental data via a download mechanism that is used on an as-needed basis.\n",
    "\n",
    "For this assignment, you will be using the `punkt`, `stopwords` and `wordnet` datasets, which are downloaded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5R-OM2e5xVF"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6QKZL4OYkft"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvcS9J9Eke4K"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgBGBHTBLl-j"
   },
   "source": [
    "## Text processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xsEqtHRT8q_"
   },
   "source": [
    "Here, we define a few functions that will be used to clean up the tweet texts. Take the time to understand what these functions are doing and how they work. You may, however, want to skim ahead to get a sense of how these functions are being used in order to better motivate your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcrugsS_iCwU"
   },
   "source": [
    "### Text tokenization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qaOWHGi3JLE"
   },
   "source": [
    "The near-universal preliminary step to natural language processing of free-form text content is the task of tokenization.\n",
    "\n",
    "Tokenization is the task of breaking a text down into its component parts, which at the sentence level in English, we think of as words -- although tokens also include things like punctuation and sometimes special-case tokens creep in as we will see is the case for Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Y57sZOd4Qqs"
   },
   "source": [
    "#### A super-simple tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3EPQCjO4URJ"
   },
   "source": [
    "Consider the following sentence (modified from the popular typing exercise in order to demonstrate some specifics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7nYRpJG3Byq"
   },
   "outputs": [],
   "source": [
    "sentence = \"The quickly browned jumping fox and the quick brown foxes jumped quickly over the lazy dogs lazily lying.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKifccHG4408"
   },
   "source": [
    "The simplest tokenizer you could build for this is probably the split function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZiF5PVe5CvG"
   },
   "outputs": [],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQATD66T5HWi"
   },
   "source": [
    ".. which works quite well in the simplest cases. But real-world text tends to not fit into simple boxes. Thus, we tend to reach for pre-built tokenizers.\n",
    "\n",
    "The NLTK word tokenizer is a good example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErEhJBTa5mXr"
   },
   "outputs": [],
   "source": [
    "nltk.tokenize.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDgft4l159Vt"
   },
   "source": [
    "A notable difference here is that punctuation is correctly tokenized, unlike with our simple `split` tokenizer.\n",
    "\n",
    "But consider this example Tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtfKSO126hi3"
   },
   "outputs": [],
   "source": [
    "example_tweet = \"hope I get a new pair of these @Nike shoes!!!! #nikelife https://www.nike.com/launch/t/womens-air-force-1-reveal-pastel-reveal\"\n",
    "nltk.tokenize.word_tokenize(example_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0mfElLs6taG"
   },
   "source": [
    "There are a few problems here, particularly the poor/improper handling of:\n",
    "\n",
    " * @mentions\n",
    " * #hashtags\n",
    " * web URLs\n",
    "\n",
    "For this reason, NLTK provides a specialized Tweet tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCfwpW1WBEZa"
   },
   "outputs": [],
   "source": [
    "nltk.TweetTokenizer().tokenize(example_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5p_HPuRhBI1n"
   },
   "source": [
    "That's better. You will further work with the Tweet Tokenizer in Homework 1. Here, let's build a simple tokenize function that can use either the word tokenizer or the Tweet tokenizer.\n",
    "\n",
    "While we are at it, we'll normalize the text to lowercase so that we can think of, e.g. \"The\" as being the same word as \"the\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZBv5GUbiF_W"
   },
   "outputs": [],
   "source": [
    "TWEET_TOKENIZER = nltk.TweetTokenizer().tokenize\n",
    "WORD_TOKENIZER = nltk.tokenize.word_tokenize\n",
    "\n",
    "def tokenize(text, lowercase=True, tweet=False):\n",
    "    \"\"\"Tokenize the text. By default, also normalizes text to lowercase.\n",
    "    Optionally uses the Tweet Tokenizer.\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    if tweet:\n",
    "        return TWEET_TOKENIZER(text)\n",
    "    else:\n",
    "        return WORD_TOKENIZER(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Sg3Z8eLih-8"
   },
   "source": [
    "### Functions that take tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0mjAeL7DaxR"
   },
   "source": [
    "After tokenizing, there are often a number of other preprocessing steps involved in preparing text data for analysis. We often consider mechanisms for text normalization.\n",
    "\n",
    "We already lowercased the text for one kind of normalization. Another thing often considered are the ideas of stemming and lemmatization. These are both approaches to dealing with variations on word forms, such as pluralization, and conjugation.\n",
    "\n",
    "Take a look at the following to get a feel for the differences. In this Lab, we will use the lemmatizer which uses more natural normalized word forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdnfTwQLM6g3"
   },
   "source": [
    "> ‚ö†Ô∏è **Caveat:** We will be using the lemmatizer in a bit of a naive way here. The WordNet lemmatizer defaults to treating words as nouns unless told otherwise. The result is that we are mainly just handling the differences in pluralization with the way we are lemmatizing. For more sophisticated lemmas, you would need to do part-of-speech tagging. You explored POS tagging in homework 2. Some extra work would be required to get the POS tags from that assignment into the form required by this lemmatizer. For purposes of this lab, we will stick with the default noun assumption.\n",
    "\n",
    "> As an example of the effects of using POS tagging, see the code snippets below. [This article at machinelearningplus.com](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/) provides a good overview of some different approaches to lemmatizing, including applying parts of speech to WordNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsE_gYZ_OPi5"
   },
   "source": [
    "### Lemmatizing with POS\n",
    "\n",
    "The following code snippets demonstrate differences in signaling the part-of-speech to the lemmatizer. The WordNet lemmatizer defaults to treating everything as nouns, which we will simply accept as good enough for the purpose of this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqq8cNsQMQUz"
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "print(\"noun:\", lemmatizer.lemmatize(\"jumping\", \"n\"))\n",
    "print(\"verb:\", lemmatizer.lemmatize(\"jumping\", \"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BrNwHQEMyM8"
   },
   "outputs": [],
   "source": [
    "print(\"noun:\", lemmatizer.lemmatize(\"lying\", \"n\"))\n",
    "print(\"verb:\", lemmatizer.lemmatize(\"lying\", \"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWeAvcPuimE6"
   },
   "outputs": [],
   "source": [
    "STEMMER = nltk.PorterStemmer()\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"Stem the tokens. I.e., remove morphological affixes and\n",
    "    normalize to standardized stem forms.\n",
    "\n",
    "    Has the side effective of producing \"unnatural\" forms due to\n",
    "    stemming standards. E.g. quickly becomes quickli\n",
    "    \"\"\"\n",
    "    return [ STEMMER.stem(token) for token in tokens ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsWmDO_6Rdao"
   },
   "outputs": [],
   "source": [
    "print(stem(tokenize(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0BqdhzMjzs1"
   },
   "outputs": [],
   "source": [
    "LEMMATIZER = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    \"\"\"Lemmatize the tokens.\n",
    "\n",
    "    Retains more natural word forms than stemming, but assumes all\n",
    "    tokens are nouns unless tokens are passed as (word, pos) tuples.\n",
    "    \"\"\"\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        if isinstance(token, str):\n",
    "            lemmas.append(LEMMATIZER.lemmatize(token)) # treats token like a noun\n",
    "        else: # assume a tuple of (word, pos)\n",
    "            lemmas.append(LEMMATIZER.lemmatize(*token))\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_Vig86yPpJD"
   },
   "outputs": [],
   "source": [
    "lemmatize([ \"foxes\", \"jumping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxnjnqUdQpuV"
   },
   "outputs": [],
   "source": [
    "lemmatize([ (\"fox\", \"n\"), (\"jumps\", \"v\") ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijVc0LI_SSJh"
   },
   "outputs": [],
   "source": [
    "print(lemmatize(tokenize(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZA7YtMFSdS7"
   },
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2q1-yPuSiId"
   },
   "source": [
    "It can be useful to remove so-called stopwords to improve the average salience of the terms we are analyzing.\n",
    "\n",
    "Stop words tend to be things like articles and conjunctions that usually don't offer a lot of value in an analysis.\n",
    "\n",
    "The NLTK has a corpus of stopwords, but we'll include the option of passing in a custom list if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDCC9UN7jBkb"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens, stopwords=None):\n",
    "    \"\"\"Remove stopwords, i.e. words that we don't want as part of our\n",
    "    analysis. Defaults to the default set of nltk english stopwords.\n",
    "    \"\"\"\n",
    "    if stopwords is None:\n",
    "        stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    return [ token for token in tokens if token not in stopwords ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysK896MCTORZ"
   },
   "outputs": [],
   "source": [
    "tokens = tokenize(sentence)\n",
    "print(tokens)\n",
    "print(remove_stopwords(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPV4VnECgSYb"
   },
   "source": [
    "### Removing hyperlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHfyiGC_gXEH"
   },
   "source": [
    "Unless your analysis involves looking at what users are linking to (a more difficult and involved task than it might seem), then you might want to simply get those links out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TRqS0G6gwdd"
   },
   "outputs": [],
   "source": [
    "def remove_links(tokens):\n",
    "    \"\"\"Removes http/s links from the tokens.\n",
    "\n",
    "    This simple implementation assumes links have been kept intact as whole\n",
    "    tokens. E.g. the way the Tweet Tokenizer works.\n",
    "    \"\"\"\n",
    "    return [ t for t in tokens\n",
    "            if not t.startswith(\"http://\")\n",
    "            and not t.startswith(\"https://\")\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wUfJBLuiBRa"
   },
   "outputs": [],
   "source": [
    "print(remove_links(tokenize(example_tweet, tweet=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odmLOVBsTjLc"
   },
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y72_TvcXbUCu"
   },
   "source": [
    "Finally, for our purposes of analysis, we are really only interested in words, not punctuation. Here, we simply remove tokens that are punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rn-aNiwLTnB5"
   },
   "source": [
    "Tweets can get pretty messy, so we've gone beyond simply removing punctation tokens and decided to clean out punctuation altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYj-wFTxlN-P"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(tokens,\n",
    "                       strip_mentions=False,\n",
    "                       strip_hashtags=False,\n",
    "                       strict=False):\n",
    "    \"\"\"Remove punctuation from a list of tokens.\n",
    "\n",
    "    Has some specialized options for dealing with Tweets:\n",
    "\n",
    "    strip_mentions=True will strip the @ off of @ mentions\n",
    "    strip_hashtags=True will strip the # from hashtags\n",
    "\n",
    "    strict=True will remove all punctuation from all tokens, not merely\n",
    "    just tokens that are punctuation per se.\n",
    "    \"\"\"\n",
    "    tokens = [t for t in tokens if t not in string.punctuation]\n",
    "    if strip_mentions:\n",
    "        tokens = [t.lstrip('@') for t in tokens]\n",
    "    if strip_hashtags:\n",
    "        tokens = [t.lstrip('#') for t in tokens]\n",
    "    if strict:\n",
    "        cleaned = []\n",
    "        for t in tokens:\n",
    "            cleaned.append(\n",
    "                t.translate(str.maketrans('', '', string.punctuation)).strip())\n",
    "        tokens = [t for t in cleaned if t]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TG1xww0lezgQ"
   },
   "outputs": [],
   "source": [
    "tokens = tokenize(example_tweet, tweet=True)\n",
    "print(tokens)\n",
    "print(remove_punctuation(tokens, strip_mentions=True, strip_hashtags=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxgNpVSIfVpc"
   },
   "outputs": [],
   "source": [
    "simple_tokens = example_tweet.split()\n",
    "print(simple_tokens)\n",
    "print(remove_punctuation(simple_tokens, strict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUz7hLKSf2eu"
   },
   "source": [
    "## Finally working with the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMgCzDn1f5pJ"
   },
   "source": [
    "Data cleanup is a big task and ultimately one of the bigger burdens of any analysis project. But, now that we have a good suite of utilities for handling our Tweets, the remainder of our work goes quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p63VPh6i0Z8"
   },
   "source": [
    "The code below will do the following for each Tweet in the dataset:\n",
    "\n",
    " * Tokenize the text using the Tweet Tokenizer\n",
    " * Remove hyperlinks\n",
    " * Remove stopwords (standard English stopwords)\n",
    " * Remove punctuation tokens and strip @ and # from hashtags and mentions (see note below)\n",
    " * Lemmatize the remaining word tokens (using default noun part-of-speech for simplicity)\n",
    "\n",
    ".. and will collect the unique words and their counts into `word_counts`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2LfXhKnmcGM"
   },
   "source": [
    "> üí° Since this is a semantic network we are building, it seems useful to, e.g., treat **@Nike** and **Nike** as the same word. Hence, `strip_mentions`, and `strip_hashtags`. In some cases, for example a mentions network, you would probably take a different approach. As you preprocess and prepare data for the task at hand, it is important to be intentional and aware of how you are handling the text with your end goals in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSrz-9ofjsKs"
   },
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "\n",
    "with gzip.open(DATA_FILE) as data_file:\n",
    "    for i, line in enumerate(data_file):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Processed {i} tweets\")\n",
    "        tweet = json.loads(line)\n",
    "        text = tweet[\"full_text\"]\n",
    "        tokens = tokenize(text, tweet=True)\n",
    "        tokens = remove_links(tokens)\n",
    "        tokens = remove_stopwords(tokens)\n",
    "        tokens = remove_punctuation(tokens, strip_mentions=True, strip_hashtags=True)\n",
    "        tokens = lemmatize(tokens)\n",
    "        for word in tokens:\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = 0\n",
    "            word_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIOWn78gn182"
   },
   "outputs": [],
   "source": [
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyJnnFYRaxWM"
   },
   "source": [
    "---\n",
    "\n",
    "## üßê Lab Quiz Question #1\n",
    "\n",
    "Precisely how many unique words are in the dataset after removing links and stopwords, and punctuation and lemmatizing the remaining tokens? Use the length of `word_counts` to determine your answer.\n",
    "\n",
    "Be sure to answer this and the remaining lab quiz questions in Lab Quiz 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_800vRAzkdz0"
   },
   "source": [
    "## Reducing the graph to the most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZFfT9jmkqyc"
   },
   "source": [
    "To keep the size of your semantic network managable, reduce the word set to just the top 1000 most popular words.\n",
    "\n",
    "To do this, you will sort the word counts by reverse value (i.e. by count from highest to lowest) and take a slice of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXBjLQVsok7-"
   },
   "outputs": [],
   "source": [
    "sorted_counts = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_words = [word for word, count in sorted_counts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMh06WnHpI1I"
   },
   "source": [
    "Let's take a look at just a few of the top words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0Jg27jIlSxU"
   },
   "outputs": [],
   "source": [
    "sorted_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L4ZajBYpnoR"
   },
   "source": [
    "Some things to note:\n",
    "\n",
    " * There appears to be some punctuation here that made it through. We will leave it as both a thought exercise to consider why these tokens are here, and how you might clean them up.\n",
    "\n",
    " * rt is right up there near the top, which is not surprising given that these are Tweets. This is an example of something you might clean up, for example, with a specialize stopword list. This cleanup is included below as a coding exercise.\n",
    "\n",
    " * While Nike and Adidas made it to the top 10, Lululemon is not here. Why might that be? (The code snippet below sheds some light) And how would you deal with this if you wanted to include Lululemon in your analysis? (Hint: think about the segmentation work you did in the Topic Modeling course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bre18RwAoVIZ"
   },
   "outputs": [],
   "source": [
    "print(\"Nike:\", word_counts[\"nike\"])\n",
    "print(\"Adidas:\", word_counts[\"adidas\"])\n",
    "print(\"Lululemon:\", word_counts[\"lululemon\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfNnCGH-rOIm"
   },
   "source": [
    "---\n",
    "\n",
    "## üßê Lab Quiz Question #2\n",
    "\n",
    "What is the most common word in the cleaned dataset? Use the sliced inspection of sorted_words above to answer the question.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arTEkZ4fsclp"
   },
   "source": [
    "## üõ† Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FD1zG0kysjkz"
   },
   "source": [
    "As mentioned above, there are a lot of \"rt\" (retweet) instances in the word set. As an exercise in developing a specialized stopword list, complete the code below to remove \"rt\" during pre-processing.\n",
    "\n",
    "The code snippet is identical to what we already did above, but this time you need to pass in a custom stop list. The custom stop list needs to include all the words that are already being stopped, plus \"rt\" as a stopword."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etlGgZnHtFP3"
   },
   "source": [
    "> ‚ö†Ô∏è Important: Due to the way `remove_stopwords` has been implemented, it is not sufficent to simply pass in [\"rt\"] as your stoplist. You'll want to be sure to include all the standard stopwords too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrBX_11gsbgO"
   },
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "\n",
    "stopwords = # TODO: Complete this code to include \"rt\" in the stopwords.\n",
    "            #       Your stopwords list should include all of the standard\n",
    "            #       nltk english stopwords, as well as the word \"rt\"\n",
    "\n",
    "\n",
    "with gzip.open(DATA_FILE) as data_file:\n",
    "    for i, line in enumerate(data_file):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Processed {i} tweets\")\n",
    "        tweet = json.loads(line)\n",
    "        text = tweet[\"full_text\"]\n",
    "        tokens = tokenize(text, tweet=True)\n",
    "        tokens = remove_links(tokens)\n",
    "        tokens = remove_stopwords(tokens, stopwords=stopwords)\n",
    "        tokens = remove_punctuation(tokens, strip_mentions=True, strip_hashtags=True)\n",
    "        tokens = lemmatize(tokens)\n",
    "        for word in tokens:\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = 0\n",
    "            word_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8Ie8EE9uIR8"
   },
   "outputs": [],
   "source": [
    "sorted_counts = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_words = [word for word, count in sorted_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMNcQ3Taukkv"
   },
   "outputs": [],
   "source": [
    "sorted_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsGrKzW9vRYI"
   },
   "source": [
    "---\n",
    "\n",
    "## üßê Lab Quiz Question #3\n",
    "\n",
    "After adding \"rt\" to the stopword list, now what is the most common word in the cleaned dataset? Use the sliced inspection of sorted_words above to answer the question.\n",
    "\n",
    "üí° Hint: This is not meant to be a trick question so much as to just be sure you are following along with understanding. If you think about it, you could probably have answered this question before even implementing the code changes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXE_X_l4wM-7"
   },
   "source": [
    "## Build and plot the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIGiHGSqwPQc"
   },
   "source": [
    "You have now done all the heavy lifting required to build the semantic network.\n",
    "\n",
    "The code below builds an undirected semantic network of co-occurring words that belong to our network of top n terms. These graphs can get kind of heavy, so start with a small graph of n=20 to keep things manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0o3dNnnx_s4"
   },
   "source": [
    "To do this, we need to:\n",
    "\n",
    " * Process each tweet in the same way we did previously\n",
    " * Determine which tokens in the Tweet belong to the top N\n",
    " * Add all of the 2-combinations (ie. co-occurrences) of included terms as an edge in the graph.\n",
    "\n",
    "We use the handy [itertools module](https://docs.python.org/3/library/itertools.html) to help us get this last thing done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZGTuFSsw0NF"
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "top_terms = sorted_words[:N]\n",
    "graph = nx.Graph()\n",
    "\n",
    "with gzip.open(DATA_FILE) as data_file:\n",
    "    for i, line in enumerate(data_file):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Processed {i} tweets\")\n",
    "        tweet = json.loads(line)\n",
    "        text = tweet[\"full_text\"]\n",
    "        tokens = tokenize(text, tweet=True)\n",
    "        tokens = remove_links(tokens)\n",
    "        tokens = remove_stopwords(tokens, stopwords=stopwords)\n",
    "        tokens = remove_punctuation(tokens, strip_mentions=True, strip_hashtags=True)\n",
    "        tokens = lemmatize(tokens)\n",
    "\n",
    "        # reduce the tweet to terms in the 1000 word network and add the\n",
    "        # term relationships to the graph\n",
    "        nodes = [t for t in tokens if t in top_terms]\n",
    "        cooccurrences = itertools.combinations(nodes, 2)\n",
    "        if i == 0:\n",
    "            print(\"Just a glimpse so you can see what the cooccurrences for a tweet look like:\")\n",
    "            cooccurrences = list(cooccurrences)\n",
    "            print(cooccurrences)\n",
    "        graph.add_edges_from(cooccurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCORIS0yb_S5"
   },
   "outputs": [],
   "source": [
    "nx.info(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpwhcmSL7Bsz"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(300, 300))\n",
    "nx.draw_networkx(graph, ax=ax, font_color=\"#FFFFFF\", font_size=20, node_size=30000, width=4, arrowsize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPGwaf6cHnTe"
   },
   "source": [
    "# Alternate Reality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dennis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dennis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dennis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 175078 tweets\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "DATA_FILE = \"nikelululemonadidas_tweets.jsonl.gz\"\n",
    "\n",
    "def load_tweets(data_file):\n",
    "    tweets = []\n",
    "    with gzip.open(data_file, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            tweets.append(tweet['full_text'])\n",
    "    return tweets\n",
    "\n",
    "tweets = load_tweets(DATA_FILE)\n",
    "print(f\"Loaded {len(tweets)} tweets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 tweets\n",
      "Processed 10000 tweets\n",
      "Processed 20000 tweets\n",
      "Processed 30000 tweets\n",
      "Processed 40000 tweets\n",
      "Processed 50000 tweets\n",
      "Processed 60000 tweets\n",
      "Processed 70000 tweets\n",
      "Processed 80000 tweets\n",
      "Processed 90000 tweets\n",
      "Processed 100000 tweets\n",
      "Processed 110000 tweets\n",
      "Processed 120000 tweets\n",
      "Processed 130000 tweets\n",
      "Processed 140000 tweets\n",
      "Processed 150000 tweets\n",
      "Processed 160000 tweets\n",
      "Processed 170000 tweets\n",
      "Sample word counts: {'ad': 12867, 'Nike': 131145, 'Women': 5335, \"'s\": 21280, 'Air': 11209, 'Uptempo': 667, '96': 263, \"'White/Opti\": 127, 'Yellow': 151, 'available': 13246}\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "def tokenize(text, tweet=False):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Removing links\n",
    "def remove_links(tokens):\n",
    "    return [token for token in tokens if not token.startswith('http')]\n",
    "\n",
    "# Removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Removing punctuation\n",
    "def remove_punctuation(tokens, strip_mentions=False, strip_hashtags=False):\n",
    "    if strip_mentions:\n",
    "        tokens = [token for token in tokens if not token.startswith('@')]\n",
    "    if strip_hashtags:\n",
    "        tokens = [token for token in tokens if not token.startswith('#')]\n",
    "    return [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Final processing function\n",
    "word_counts = {}\n",
    "preprocessed_tweets = []\n",
    "\n",
    "with gzip.open(DATA_FILE, 'rt', encoding='utf-8') as data_file:\n",
    "    for i, line in enumerate(data_file):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Processed {i} tweets\")\n",
    "        tweet = json.loads(line)\n",
    "        text = tweet[\"full_text\"]\n",
    "        tokens = tokenize(text, tweet=True)\n",
    "        tokens = remove_links(tokens)\n",
    "        tokens = remove_stopwords(tokens)\n",
    "        tokens = remove_punctuation(tokens, strip_mentions=True, strip_hashtags=True)\n",
    "        tokens = lemmatize(tokens)\n",
    "        preprocessed_tweets.append(tokens)\n",
    "        for word in tokens:\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = 0\n",
    "            word_counts[word] += 1\n",
    "\n",
    "# Display some of the word counts\n",
    "print(\"Sample word counts:\", dict(list(word_counts.items())[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Sample 10% of the tweets for visualization\n",
    "sampled_tweets = random.sample(preprocessed_tweets, len(preprocessed_tweets) // 10)\n",
    "\n",
    "# Get the top N most frequent words\n",
    "N = 100\n",
    "most_frequent_words = sorted(word_counts, key=word_counts.get, reverse=True)[:N]\n",
    "\n",
    "# Filter the sampled tweets to include only the most frequent words\n",
    "filtered_tweets = [[word for word in tweet if word in most_frequent_words] for tweet in sampled_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 100\n",
      "Number of edges: 2558\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "# Build semantic network\n",
    "G = nx.Graph()\n",
    "for tokens in filtered_tweets:\n",
    "    for pair in itertools.combinations(tokens, 2):\n",
    "        if G.has_edge(pair[0], pair[1]):\n",
    "            G[pair[0]][pair[1]]['weight'] += 1\n",
    "        else:\n",
    "            G.add_edge(pair[0], pair[1], weight=1)\n",
    "\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Only keep edges with weight above a certain threshold\n",
    "weight_threshold = 5\n",
    "simplified_G = nx.Graph(((u, v, d) for u, v, d in G.edges(data=True) if d['weight'] > weight_threshold))\n",
    "\n",
    "# Visualize the network\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(simplified_G, k=0.1)\n",
    "nx.draw(simplified_G, pos, with_labels=True, node_size=50, font_size=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßê Lab Quiz Question #1\n",
    "\n",
    "Precisely how many unique words are in the dataset after removing links and stopwords, and punctuation and lemmatizing the remaining tokens? Use the length of `word_counts` to determine your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_count = len(word_counts)\n",
    "print(f\"Number of unique words: {unique_words_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
